Vertex AI
Documentation
Generative AI
Was this helpful?
Send feedback
Tune a model
bookmark_border
On this page
Prepare your model tuning dataset
Upload the dataset to a Cloud Storage bucket
Create a model tuning job
Example curl command
What's next
Use model tuning to improve a model's performance on specific tasks or desired behaviors by giving the model many examples illustrating that task or behavior. Tuning is required when you want the model to learn a specific behavior or niche. For more information, see Scenarios to use model tuning.
Prepare your model tuning dataset
Your dataset must include at least 10 examples that align with the task that you want the model to perform. For better results, we recommend at least 100 examples. For more information, see Prepare your model tuning dataset.
This example can be used as your sample dataset.
{"input_text": "question: How many people live in Beijing? context: With over 21 million residents, Beijing is the world's most populous national capital city and is China's second largest city after Shanghai. It is located in Northern China, and is governed as a municipality under the direct administration of the State Council with 16 urban, suburban, and rural districts.[14] Beijing is mostly surrounded by Hebei Province with the exception of neighboring Tianjin to the southeast; together, the three divisions form the Jingjinji megalopolis and the national capital region of China.", "output_text": "over 21 million people"}
{"input_text": "question: How many parishes are there in Louisiana? context: The U.S. state of Louisiana is divided into 64 parishes (French: paroisses) in the same manner that 48 other states of the United States are divided into counties, and Alaska is divided into boroughs.", "output_text": "64"}
{"input_text": "question: How many churches in Texas? context: In 2010, there were a number of religious congregations in the state of Texas.", "output_text": "27,848"}
{"input_text": "question: How many lakes in North Dakota? context: North Dakota has many lakes and rivers offering exciting action for walleye, northern pike, perch, bass, salmon, catfish and other game fish with seasons for most species open year-round.", "output_text": "400"}
{"input_text": "question: How many rivers in the United States? context: The United States of America has over 250,000 rivers, with a total of about 3,500,000 miles of rivers. The longest river in the USA is the Missouri River (it is a tributary of the Mississippi River and is 2,540 miles long), but the biggest in terms of water volume is the deeper Mississippi River", "output_text": "over 250,000"}
{"input_text": "question: How many mountains in Oregon? context: There are 4760 named mountain ranges in Oregon and approximately 3,764 mountains altogether.", "output_text": "3,764"}
{"input_text": "question: How many small businesses in the state of Vermont? context: Vermont has 78,883 small businesses, most of which are sole proprietors. Vermont small businesses employ 157,131 workers, which is 60.2% of the state's workforce. The top three industries for small business in Vermont are professional, scientific, and technical services; construction; and retail.", "output_text": "78,883"}
{"input_text": "question: How many states grow apples? context: 2,500 varieties of apples are grown in the United States. 7,500 varieties of apples are grown throughout the world. 100 varieties of apples are grown commercially in the United States. Apples are grown commercially in 36 states.", "output_text": "36"}
{"input_text": "question: How many states grow mangos? context: Because mangos need a tropical climate to flourish, only Florida, California, Hawaii, and Puerto Rico grow mangos.", "output_text": "4"}
{"input_text": "question: How often does it storm in Missouri? context: Thunderstorms normally occur between 40 and 50 days per year. During any year, there are usually a few of these thunderstorms that are severe, and produce large hail and damaging winds. Tornadoes have produced extensive damage and loss of life in the St. Louis area.", "output_text": "Thunderstorms normally occur between 40 and 50 days per year."}
Upload the dataset to a Cloud Storage bucket
Follow these steps to upload your dataset:
Create a new Cloud Storage bucket, or use an existing bucket to store your dataset file. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to run model tuning.
After your bucket is ready, upload your dataset file to the bucket.
Create a model tuning job
Select a tab, and follow the instructions to run the sample.
REST
Vertex AI SDK for Python
Node.js
Java
Console
To create a model tuning job, send a POST request by using the pipelineJobs method.
Before using any of the request data, make the following replacements:
PIPELINEJOB_DISPLAYNAME: A display name for the pipelineJob.
OUTPUT_DIR: The URI of the bucket to output pipeline artifacts to.
PROJECT_ID: Your project ID.
MODEL_DISPLAYNAME: A display name for the model uploaded (created) by the pipelineJob.
DATASET_URI: URI of your dataset file.
TUNING_LOCATION: The region where model tuning takes place. Supported regions are:
us-central1: Uses eight A100 80 GB GPUs. Make sure you have enough quota. Supports CMEK and VPC‑SC.
europe-west4: Uses 64 cores of the TPU v3 pod. Make sure you have enough quota. CMEK isn't supported, but VPC‑SC is supported.
LARGE_MODEL_REFERENCE: Name of the foundation model to tune. The options are:
text-bison
chat-bison
DEFAULT_CONTEXT (chat only): The context that applies to all tuning examples in the tuning dataset. Setting the `context` field in an example overrides the default context.
STEPS: The number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location:
us-central1 has a batch size of 8.
europe-west4 has a batch size of 24.
If there are 240 examples in a training dataset, in europe-west4, it takes 240 / 24 = 10 steps to process the entire dataset once. In us-central1, it takes 240 / 8 = 30 steps to process the entire dataset once.
LEARNING_RATE_MULTIPLIER: A multiplier to apply to the recommended learning rate. To use the recommended learning rate, use 1.0.
EVAL_DATASET_URI (text only): (optional) The URI of the JSONL file that contains the evaluation dataset for batch prediction and evaluation. For more information, see Dataset format for tuning a code model. The evaluation dataset requires between ten and 250 examples.
EVAL_INTERVAL (text only): (optional, default 20) The number of tuning steps between each evaluation. Because the evaluation runs on the entire evaluation dataset, a smaller evaluation interval results in a longer tuning time. For example, if steps is 200 and EVAL_INTERVAL is 100, then you will get only two data points for the evaluation metrics. This parameter requires that the evalutation_data_uri is set.
ENABLE_EARLY_STOPPING: (optional, default true) A boolean that, if set to true, stops tuning before completing all the tuning steps if model performance, as measured by the accuracy of predicted tokens, does not improve enough between evaluations runs. If false, tuning continues until all the tuning steps are complete. This parameter requires that the evaluation_data_uri is set.
TENSORBOARD_RESOURCE_ID (text only): (optional) The ID of the Vertex AI TensorBoard instance to create an experiment on after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.
ENCRYPTION_KEY_NAME: (optional) The fully qualified name of the CMEK key that you want to use for data encryption. Available in supported regions only.
TEMPLATE_URI: The tuning template to use depends on the model that you're tuning:
Text model: https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0
Chat model: https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-chat-model/v3.0.0
SERVICE_ACCOUNT: (optional) The service account that Vertex AI uses to run your pipeline job. By default, your project's Compute Engine default service account (PROJECT_NUMBER‑compute@developer.gserviceaccount.com) is used. Learn more about attaching a custom service account.
HTTP method and URL:
POST https://
TUNING_LOCATION-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/
TUNING_LOCATION/pipelineJobs
Request JSON body:
{
  "displayName": "
PIPELINEJOB_DISPLAYNAME",
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://
OUTPUT_DIR",
    "parameterValues": {
      "project": "
PROJECT_ID",
      "model_display_name": "
MODEL_DISPLAYNAME",
      "dataset_uri": "gs://
DATASET_URI",
      "location": "us-central1",
      "large_model_reference": "
LARGE_MODEL_REFERENCE",
      "default_context": "
DEFAULT_CONTEXT (chat only)",
      "train_steps": 
STEPS,
      "learning_rate_multiplier": 
LEARNING_RATE_MULTIPLIER,
      "evaluation_data_uri": "gs://
EVAL_DATASET_URI (text only)",
      "evaluation_interval": 
EVAL_INTERVAL (text only),
      "enable_early_stopping": 
ENABLE_EARLY_STOPPING,
      "tensorboard_resource_id": "
TENSORBOARD_ID (text only)",
      "encryption_spec_key_name": "
ENCRYPTION_KEY_NAME"
    }
  },
  "encryptionSpec": {
    "kmsKeyName": "
ENCRYPTION_KEY_NAME"
  },
  "serviceAccount": "
SERVICE_ACCOUNT",
  "templateUri": "
TEMPLATE_URI"
}
To send your request, choose one of these options:
curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login, or by using Cloud Shell, which automatically logs you into the gcloud CLI. You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:
curl -X POST \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    -H "Content-Type: application/json; charset=utf-8" \
    -d @request.json \
    "https://
TUNING_LOCATION-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/
TUNING_LOCATION/pipelineJobs"








You should receive a JSON response similar to the following. Note that pipelineSpec has been truncated to save space.
Response














































































































































































































































Note: All data is processed in the same region as the pipeline job (either us-central1 or europe-west4). After the job is complete, the tuned model is uploaded to us-central1.
Example curl command
PROJECT_ID=myproject
DATASET_URI=gs://my-gcs-bucket-uri/dataset
OUTPUT_DIR=gs://my-gcs-bucket-uri/output

curl \
-X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
"https://europe-west4-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/europe-west4/pipelineJobs?pipelineJobId=tune-large-model-$(date +%Y%m%d%H%M%S)" -d \
$'{
  "displayName": "tune-llm",
  "runtimeConfig": {
    "gcsOutputDirectory": "'${OUTPUT_DIR}'",
    "parameterValues": {
      "project": "'${PROJECT_ID}'",
      "model_display_name": "The display name for your model in the UI",
      "dataset_uri": "'${DATASET_URI}'",
      "location": "us-central1",
      "large_model_reference": "text-bison@001",
      "train_steps": 300,
      "learning_rate_multiplier": 1.0
    }
  },
  "templateUri": "https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0"
}'
What's next
Learn about designing text prompts. and text chat prompts.
Learn how to test prompts in Generative AI Studio.
Learn about text embeddings.
Learn about responsible AI best practices and Vertex AI's safety filters.
Was this helpful?
Send feedback