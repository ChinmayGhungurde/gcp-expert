Vertex AI
Documentation
Generative AI
Was this helpful?
Send feedback
Evaluate model performance
On this page
How model evaluation works
Supported models
Prepare evaluation dataset
Dataset format
Upload evaluation dataset to Cloud Storage
Perform model evaluation
What's next
Preview
This feature is a Preview offering, subject to the Pre-GA Offerings Terms of the GCP Service Specific Terms. Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the launch stage descriptions. Further, by using the PaLM API on Vertex AI, you agree to the Generative AI Preview terms and conditions (Preview Terms).
For PaLM APIs on Vertex AI that are not GA, you can process personal data as outlined in the Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).
You can evaluate the performance of foundation models and your tuned generative AI models on Vertex AI. The models are evaluated using a set of metrics against an evaluation dataset that you provide. This page explains how model evaluation works, how to create and format the evaluation dataset, and how to perform evaluation by using the Google Cloud console, Vertex AI API, or the Vertex AI SDK for Python.
How model evaluation works
To evaluate the performance of a model, you first create an evaluation dataset that contains prompts and ground truth pairs. For each pair, the prompt is the prompt that you want to evaluate and the ground truth is the ideal response for that prompt. During evaluation, the prompt of each pair in the evaluation dataset is passed to the model to produce an output. The output generated by the model and the ground truth from the evaluation dataset are used to compute the evaluation metrics.
The metrics used for evaluation depends on the task that you are evaluating. The following table shows the supported tasks and the metrics used to evaluate each tasks:
Task Metric
Classification Micro-F1, Macro-F1, Per class F1
Summarization ROUGE-L
Question answering Exact Match
Text generation BLEU, ROUGE-L
Supported models
Model evaluation is supported for the base and tuned versions of the following Vertex AI foundation models:
text-bison
Prepare evaluation dataset
The evaluation dataset used for model evaluation includes prompt and ground truth pairs that align with the task that you want to evaluate. Your dataset must include a minimum of one prompt and ground truth pair, but we recommend at least 10 pairs for meaningful metrics. Generally speaking, the more examples you give, the more meaningful the results.
Dataset format
Your evaluation dataset must be in JSON Lines (JSONL) format where each line contains a single prompt and ground truth pair specified in the prompt and ground_truth fields, respectively. The prompt field contains the prompt that you want to evaluate and the ground_truth field contains the ideal response for the prompt.
The maximum token length for prompt is 8,192 and the maximum token length for ground_truth is 1,024.
Example evaluation datasets
Classification
Question & answering
General text generation
More
{"prompt": "Multi-choice problem: What is the topic of this text? OPTIONS: -
nature - news - politics - sports - health - startups TEXT: #DYK In 2015, the
world produced 322M tonnes of plastic. That equals 900 Empire State Buildings!
Act:\u2026 https://t.co/qGrpumIN20", "ground_truth": "nature"}
{"prompt": "Multi-choice problem: What is the topic of this text? OPTIONS: -
nature - news - politics - sports - health - startups TEXT: Do you agree with
Chris Pratt? https://t.co/1q43CvIWAY https://t.co/zcKnTa9hKS", "ground_truth":
"news"}
{"prompt": "Multi-choice problem: What is the topic of this text? OPTIONS: -
nature - news - politics - sports - health - startups TEXT: Cahill and Diego
Costa aren't far away from getting their head on Hazard's corner. Meanwhile
Ibe has replaced Pugh for Bournemouth. #BOUCHE", "ground_truth": "sports"}
{"prompt": "Multi-choice problem: What is the topic of this text? OPTIONS: -
nature - news - politics - sports - health - startups TEXT: Protecting our
oceans is integral to protecting the global environment #EarthDay
https://t.co/CN5NHKFbsB", "ground_truth": "nature"}
{"prompt": "Multi-choice problem: What is the topic of this text? OPTIONS: -
nature - news - politics - sports - health - startups TEXT: Scientists have
found that water is gushing across Antarctica \u2014 more than they ever
realized:\u2026 https://t.co/ttBGA15w5G", "ground_truth": "news"}















































































Upload evaluation dataset to Cloud Storage
You can either create a new Cloud Storage bucket or use an existing one to store your dataset file. The bucket must be in the same region as the model.
After your bucket is ready, upload your dataset file to the bucket.
Perform model evaluation
You can evaluate models by using the Google Cloud console, REST API, or the Vertex AI SDK for Python.
REST
Console
To create a model evaluation job, send a POST request by using the pipelineJobs method.
Before using any of the request data, make the following replacements:
PROJECT_ID: The Google Cloud project that runs the pipeline components.
PIPELINEJOB_DISPLAYNAME: A display name for the pipelineJob.
LOCATION: The region to run the pipeline components. Currently, only us-central1 is supported.
DATASET_URI: The Cloud Storage URI of your reference dataset. You can specify one or multiple URIs. This parameter supports wildcards. To learn more about this parameter, see InputConfig.
OUTPUT_DIR: The Cloud Storage URI to store evaluation output.
MODEL_NAME: Specify a publisher model or a tuned model resource as follows:
Publisher model: publishers/google/models/MODEL@MODEL_VERSION
Example: publishers/google/models/text-bison@001
Tuned model: projects/PROJECT_NUMBER/locations/LOCATION/models/ENDPOINT_ID
Example: projects/123456789012/locations/us-central1/models/1234567890123456789
The evaluation job doesn't impact any existing deployments of the model or their resources.
EVALUATION_TASK: The task that you want to evaluate the model on. The evaluation job computes a set of metrics relevant to that specific task. Acceptable values include the following:
summarization
question-answering
text-generation
classification
INSTANCES_FORMAT: The format of your dataset. Currently, only jsonl is supported. To learn more about this parameter, see InputConfig.
PREDICTIONS_FORMAT: The format of the evaluation output. Currently, only jsonl is supported. To learn more about this parameter, see InputConfig.
MACHINE_TYPE: (Optional) The machine type for running the evaluation job. The default value is e2-highmem-16. For a list of supported machine types, see Machine types.
SERVICE_ACCOUNT: (Optional) The service account to use for running the evaluation job. To learn how to create a custom service account, see Configure a service account with granular permissions. If unspecified, the Vertex AI Custom Code Service Agent is used.
NETWORK: (Optional) The fully qualified name of the Compute Engine network to peer the evaluatiuon job to. The format of the network name is projects/PROJECT_NUMBER/global/networks/NETWORK_NAME. If you specify this field, you need to have a VPC Network Peering for Vertex AI. If left unspecified, the evaluation job is not peered with any network.
KEY_NAME: (Optional) The name of the customer-managed encryption key (CMEK). If configured, resources created by the evaluation job is encrypted using the provided encryption key. The format of the key name is projects/PROJECT_ID/locations/REGION/keyRings/KEY_RING/cryptoKeys/KEY. The key needs to be in the same region as the evaluation job.
HTTP method and URL:
POST https://
LOCATION-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/
LOCATION/pipelineJobs
Request JSON body:
{
  "displayName": "
PIPELINEJOB_DISPLAYNAME",
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://
OUTPUT_DIR",
    "parameterValues": {
      "project": "
PROJECT_ID",
      "location": "
LOCATION",
      "batch_predict_gcs_source_uris": ["gs://
DATASET_URI"],
      "batch_predict_gcs_destination_output_uri": "gs://
OUTPUT_DIR",
      "model_name": "
MODEL_NAME",
      "evaluation_task": "
EVALUATION_TASK",
      "batch_predict_instances_format": "
INSTANCES_FORMAT",
      "batch_predict_predictions_format: "
PREDICTIONS_FORMAT",
      "machine_type": "
MACHINE_TYPE",
      "service_account": "
SERVICE_ACCOUNT",
      "network": "
NETWORK",
      "encryption_spec_key_name": "
KEY_NAME"
    }
  },
  "templateUri": "https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-text-generation-pipeline/1.0.1"
}
To send your request, choose one of these options:
curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login, or by using Cloud Shell, which automatically logs you into the gcloud CLI. You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:
curl -X POST \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    -H "Content-Type: application/json; charset=utf-8" \
    -d @request.json \
    "https://
LOCATION-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/
LOCATION/pipelineJobs"








You should receive a JSON response similar to the following. Note that pipelineSpec has been truncated to save space.
Response
Example curl command
PROJECT_ID=myproject
REGION=us-central1
MODEL_NAME=publishers/google/models/text-bison@001
TEST_DATASET_URI=gs://my-gcs-bucket-uri/dataset.jsonl
OUTPUT_DIR=gs://my-gcs-bucket-uri/output

curl \
-X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
"https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/pipelineJobs" -d \
$'{
  "displayName": "evaluation-llm-text-generation-pipeline",
  "runtimeConfig": {
    "gcsOutputDirectory": "'${OUTPUT_DIR}'",
    "parameterValues": {
      "project": "'${PROJECT_ID}'",
      "location": "'${REGION}'",
      "batch_predict_gcs_source_uris": ["'${TEST_DATASET_URI}'"],
      "batch_predict_gcs_destination_output_uri": "'${OUTPUT_DIR}'",
      "model_name": "'${MODEL_NAME}'",
    }
  },
  "templateUri": "https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-text-generation-pipeline/1.0.1"
}'
What's next
Learn how to tune a foundation model.
Was this helpful?
Send feedback