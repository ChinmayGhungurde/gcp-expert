Vertex AI
Documentation
Generative AI
Was this helpful?
Send feedback
Responsible AI for Imagen
On this page
Safety filters
Limitations
Image generation limitations
Image editing limitations
Visual captioning limitations
Visual Question Answering (VQA) limitations
Recommended practices
Additional resources
Imagen on Vertex AI brings Google's state of the art generative AI capabilities to application developers. As an early-stage technology, Imagen on Vertex AI's evolving capabilities and uses create potential for misapplication, misuse, and unintended or unforeseen consequences. For example, Imagen on Vertex AI could generate output that you don't expect, such as images that are offensive, insensitive, or contextually incorrect.
Given these risks and complexities, Imagen on Vertex AI is designed with Google's AI Principles in mind. However, it is important for developers to understand and test their models to deploy them safely and responsibly. To aid developers, Imagen on Vertex AI has built-in safety filters to help customers block potentially harmful outputs within their use case. Please see the safety filters section for more.
When Imagen on Vertex AI is integrated into a customer's unique use case and context, additional responsible AI considerations and model limitations may need to be considered. We encourage customers to leverage fairness, interpretability, privacy and security recommended practices.
Safety filters
Text prompts provided as inputs and images (generated or uploaded) through the Imagen on Vertex AI are assessed against a list of safety attributes, which include 'harmful categories' (for example, violence, sexual, derogatory, and toxic). These safety attributes aim to filter out (generated or uploaded) content that violates our Acceptable Use Policy (AUP), Generative AI Prohibited Use Policy or our AI Principles.
Each safety attribute has an associated confidence score. This means that a model output is blocked if it exceeds a safety attribute's designated confidence score. If the model responds to a request with "X images were filtered out because they violated Google's Responsible AI practices" it means that either the input or the output is triggering a safety filter.
Limitations
Image generation limitations
Bias amplification: While Imagen on Vertex AI can generate high-quality images, there may be potential biases in the generated content. Images generated rely on the product's training data, which can unintentionally include biases that may perpetuate stereotypes or discriminate against certain groups. Careful monitoring and evaluation are necessary to ensure the outputs align with Google's Acceptable Use Policy and your use case.
Transparency and disclosure: It can be difficult for users to differentiate between AI generated Imagery and non AI generated imagery. When using AI-generated images within your use case, it is important to clearly disclose to users that the images have been generated by an AI system to ensure transparency and maintain trust in the process. We've applied metadata labeling to AI-generated images to help combat the risk of misinformation and as part of our responsible approach to AI.
Insufficient context: Imagen on Vertex AI may lack the contextual understanding required to generate images that are appropriate for all situations or audiences within your use case. Be sure to check that your generated images align with your desired context, purpose, and intended audience.
Image editing limitations
Misrepresentation and authenticity: Editing images using Imagen on Vertex AI can result in misrepresentation or manipulation of images, potentially leading to the creation of deceptive or misleading content. It is important to ensure that the editing process is used responsibly, without compromising the authenticity and truthfulness of the images edited. We've applied metadata labeling to AI-edited images to help combat the risk of misinformation and as part of our responsible approach to AI.
Visual captioning limitations
Accuracy and context sensitivity: Visual captioning may encounter challenges in accurately describing complex or ambiguous images. The generated descriptions may not always capture the complete context or nuances of the visual content. It is important to acknowledge that automated captioning systems have limitations in understanding images with varying levels of complexity, and their descriptions should be used with caution, particularly in critical or sensitive contexts.
Ambiguity and subjective interpretations: Images can often be open to multiple interpretations, and the generated captions may not always align with human understanding or expectations. Different individuals may perceive and describe images differently based on their subjective experiences and cultural backgrounds. It is crucial to consider the potential for ambiguity and subjectivity in image descriptions and provide additional context or alternative interpretations where necessary.
Accessibility considerations: While automated image captions can support accessibility by providing descriptions for visually impaired individuals, it is important to recognize that they may not fully replace human-generated alt-text or descriptions tailored to specific accessibility needs. Automated captions may lack the level of detail or contextual understanding necessary for certain accessibility use cases.
Visual Question Answering (VQA) limitations
Overconfidence and uncertainty: VQA models may sometimes provide answers with unwarranted confidence, even when the correct answer is uncertain or ambiguous. It is essential to communicate the model's uncertainty and provide appropriate confidence scores or alternative answers when there is ambiguity, rather than conveying a false sense of certainty.
Recommended practices
To utilize this technology safely and responsibly, it is also important to consider other risks specific to your use case, users, and business context in addition to built-in technical safeguards.
We recommend taking the following steps:
Assess your application's security risks.
Consider adjustments to mitigate safety risks.
Perform safety testing appropriate to your use case.
Solicit user feedback and monitor content.
Additional resources
Learn about Responsible AI for Large Language Models (LLMs).
Learn more about Google's recommendations for Responsible AI practices.
Read our blog, A shared agenda for responsible AI progress
Give feedback on Imagen on Vertex AI
If you receive an output or response that is inaccurate or that you feel is unsafe, you can let us know by submitting feedback. Your feedback can help improve Imagen on Vertex AI and broader Google efforts in AI.
Because feedback may be human readable, please do not submit data that contains personal, confidential, or sensitive information.
Previous
arrow_back
Usage guidelines
Was this helpful?
Send feedback