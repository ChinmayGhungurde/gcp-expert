Vertex AI
Documentation
Generative AI
Was this helpful?
Send feedback
Tune text embeddings
bookmark_border
On this page
Use case for tuning an embedding model
Tuning workflow
Prepare your embeddings dataset
Dataset format for tuning an embeddings model
Create a embedding model tuning job
View tuned models in Model Registry
Deploy your model
Get predictions on deployed model
Preview
This feature is a Preview offering, subject to the Pre-GA Offerings Terms of the GCP Service Specific Terms. Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the launch stage descriptions. Further, by using the PaLM API on Vertex AI, you agree to the Generative AI Preview terms and conditions (Preview Terms).
For PaLM APIs on Vertex AI that are not GA, you can process personal data as outlined in the Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).
This page shows you how to tune the text embedding model, textembedding-gecko. The textembedding-gecko model is a foundation model that's been trained on a large set of public text data. If you have a unique use case which requires your own specific training data you can use model tuning. After you tune a foundation embedding model, the model should be catered for your use case. Tuning is supported for stable versions of the text embedding model.
Text embedding models support supervised tuning. Supervised tuning uses labeled examples that demonstrate the type of output you'd like from your text embedding model during inference. Text embedding models don't support tuning by using Reinforcement learning from human feedback(RLHF).
To learn more about model tuning, see How model tuning works.
Use case for tuning an embedding model
Tuning a text embeddings model can enable your model to adapt to the embeddings to a specific domain or task. This can be useful if the pre-trained embeddings model is not well-suited to your specific needs. For example, you might fine-tune an embeddings model on a specific dataset of customer support tickets for your company. This could help a chatbot understand the different types of customer support issues your customers typically have, and be able to answer their questions more effectively. Without tuning, textembedding-gecko can't know the specifics of your customer support tickets or the solutions to specific problems for your product.
Tuning workflow
The model tuning workflow on Vertex AI for textembedding-gecko is as follows:
Prepare your model tuning dataset.
Upload the model tuning dataset to a Cloud Storage bucket.
Create a model tuning job.
Deploy the tuned model to a Vertex AI endpoint of the same name. Unlike text or Codey model tuning jobs, a text embedding tuning job doesn't deploy your tuned models to a Vertex AI endpoint.
Prepare your embeddings dataset
The dataset used to tune an embeddings model includes data that align with the task that you want the model to perform.
Dataset format for tuning an embeddings model
The training dataset consists of the following files, which need to be in Cloud Storage. The path of the files are defined by parameters when launching the tuning pipeline. The four required files are the corpus file, query file, training labels, and test labels.
Corpus file: The path is defined by parameter corpus_path. It's a JSONL file where each line has the fields _id, title, and text with string values. _id and text are required, while title is optional. For example:
{"_id": "doc1", "title": "Get an introduction to generative AI on Vertex AI", "text": "Vertex AI's Generative AI Studio offers a Google Cloud console tool for rapidly prototyping and testing generative AI models. Learn how you can use Generative AI Studio to test models using prompt samples, design and save prompts, tune a foundation model, and convert between speech and text."}
{"_id": "doc2", "title": "Use gen AI for summarization, classification, and extraction", "text": "Learn how to create text prompts for handling any number of tasks with Vertex AI's generative AI support. Some of the most common tasks are classification, summarization, and extraction. Vertex AI's PaLM API for text lets you design prompts with flexibility in terms of their structure and format."}
{"_id": "doc3", "title": "Custom ML training overview and documentation", "text": "Get an overview of the custom training workflow in Vertex AI, the benefits of custom training, and the various training options that are available. This page also details every step involved in the ML training workflow from preparing data to predictions."}
Query file: The query file contains your example queries. The path is defined by the parameter queries_path. The query file is in JSONL format and has the same fields as the corpus file. For example:
{"_id": "query1", "title": "generative AI on Vertex", "text": "Does Vertex support generative AI?"}
{"_id": "query2", "text": "What can I do with Vertex GenAI offerings?"}
{"_id": "query3", "text": "How do I train my models using Vertex?"}
Training labels: The path is defined by the parameter train_label_path. The train_label_path is the Cloud Storage URI to the train label data location and is specified when you create your tuning job. The labels need to be a TSV file with a header. A subset of the queries and the corpus need be included in your training labels file. The file must have the columns query-id, corpus-id and score. The query-id is a string that matches the _id key from the query file, the corpus-id is a string that matches the _id in the corpus file. Score is a non-negative integer value. Any score greater than zero indicates that the document is related to the query. Larger numbers indicate a greater level of relevance. If the score is emitted, the default value is 1. For example:
query-id  corpus-id   score
query1    doc1    1
query2    doc2    1
query3    doc3    2
Note: Ensure the separators in this file are tab characters (\t). Some browsers and text editors may replace tabs with a sequence of spaces.
Test labels: The test labels file path is specified by the test_label_path parameter. test_label_path is the Cloud Storage URI to the test label data location and is specified when you create your tuning job. This file has the same format as the training labels.
Create a embedding model tuning job
The following shows you how to create an embedding model tuning job using REST API commands. You can't tune an embedding model using the Google Cloud console at this time.
REST
Before using any of the request data, make the following replacements:
DISPLAY_NAME: A display name for the pipelineJob.
PIPELINE_SCRATCH_PATH: Path for the pipeline output artifacts.
PROJECT_ID: your Google Cloud project ID.
QUERIES_PATH: The URI of the Cloud Storage bucket to store queries
CORPUS_PATH: The Cloud Storage URI for the corpus data. .
TRAIN_LABEL_PATH: The Cloud Storage URI to the train label data location.
TEST_LABEL_PATH: The Cloud Storage URI to the test label data location.
BATCH_SIZE: The training batch size.
ITERATIONS: The number of steps to perform model tuning.
HTTP method and URL:
POST https://us-central1-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/us-central1/pipelineJobs
Request JSON body:
{
  "displayName": "
DISPLAY_NAME",
  "runtimeConfig": {
    "gcsOutputDirectory": "
PIPELINE_SCRATCH_PATH",
    "parameterValues": {
      "project": "
PROJECT_ID",
      "location": "
us-central1",
      "queries_path": "
QUERIES_PATH",
      "corpus_path": "
CORPUS_PATH",
      "train_label_path": "
TRAIN_LABEL_PATH",
      "test_label_path": "
TEST_LABEL_PATH",
      "batch_size": "
BATCH_SIZE",
      "iterations": "
ITERATIONS"
    }
  },
  "templateUri": "https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.0"
}
To send your request, expand one of these options:
curl (Linux, macOS, or Cloud Shell)




PowerShell (Windows)








You should receive a JSON response similar to the following:
Response
After launching the pipeline, follow the progress of your tuning job through the Google Cloud console.
Go to Google Cloud console
Example curl command
PROJECT_ID=
PROJECT_ID
PIPELINE_SCRATCH_PATH=
PIPELINE_SCRATCH_PATH
QUERIES_PATH=
QUERIES_PATH
CORPUS_PATH=
CORPUS_PATH
TRAIN_LABEL_PATH=
TRAIN_LABEL_PATH
TEST_LABEL_PATH=
TEST_LABEL_PATH
BATCH_SIZE=
BATCH_SIZE
ITERATIONS=
ITERATIONS


curl -X POST  \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json; charset=utf-8" \
"https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/pipelineJobs?pipelineJobId=tune-text-embedding-$(date +%Y%m%d%H%M%S)" \
-d '{
  "displayName": "tune-text-embedding-model",
  "runtimeConfig": {
    "gcsOutputDirectory": "'${PIPELINE_SCRATCH_PATH}'",
    "parameterValues": {
      "project":  "'${PROJECT_ID}'",
      "location": "us-central1",
      "queries_path":  "'${QUERIES_PATH}'",
      "corpus_path":  "'${CORPUS_PATH}'",
      "train_label_path":  "'${TRAIN_LABEL_PATH}'",
      "test_label_path":  "'${TEST_LABEL_PATH}'",
      "batch_size":  "'${BATCH_SIZE}'",
      "iterations":  "'${ITERATIONS}'"
    }
  },
  "templateUri": "https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.0"
}'
View tuned models in Model Registry
You can view a list of models in your current project, including your tuned models, by using the Google Cloud console.
To view your tuned models in the Google Cloud console, go to the Vertex AI Model Registry page.
Go to Vertex AI Model Registry
Deploy your model
When your tuning job completes, the tuned model isn't deployed to an endpoint. After you've tuned the embeddings model, you need to deploy your model. To deploy your tuned embeddings model, see Deploy a model to an endpoint.
Unlike foundation models, tuned text embedding models are managed by the user. This includes managing serving resources, like machine type and accelerators. To prevent out-of-memory errors during prediction, it's recommended that you deploy using the NVIDIA_TESLA_A100 GPU type, which can support batch sizes up to 5 for any input length.
Similar to the textembedding-gecko foundation model, your tuned model supports up to 3072 tokens and can truncate longer inputs.
Get predictions on deployed model
Example curl command
ENDPOINT_URI=https://us-central1-aiplatform.googleapis.com
PROJECT_ID=<your-project>
LOCATION=us-central1
MODEL_ENDPOINT=<created-vertex-endpoint-id>

curl -X POST -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    -H "Content-Type: application/json"  \
    ${ENDPOINT_URI}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/MODEL_ENDPOINT:predict \
    -d '{
  "instances": [
    {
      "content": "Dining in New York City"
    },
    {
      "content": "Best resorts on the east coast"
    }
  ]
}'
Example output
Note: The output of a prediction request to the deployed tuned textembedding-gecko model is not the same as the text embedding API output.
{
 "predictions": [
   [ ... ],
   [ ... ],
   ...
 ],
 "deployedModelId": "...",
 "model": "projects/.../locations/us-central1/models/...",
 "modelDisplayName": "tuned-text-embedding-model",
 "modelVersionId": "1"
}
What's next
Learn how to tune a foundation model.
Learn about responsible AI best practices and Vertex AI's safety filters.
Was this helpful?
Send feedback