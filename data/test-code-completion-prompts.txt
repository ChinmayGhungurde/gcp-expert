Vertex AI
Documentation
Generative AI
Was this helpful?
Send feedback
Test code completion prompts
bookmark_border
On this page
Test code completion prompts
Stream response from code model
What's next
To design a prompt that works well, test different versions of the prompt and experiment with prompt parameters to determine what results in the optimal response. You can test prompts programmatically with the Codey APIs and in the Google Cloud console with Generative AI Studio.
Test code completion prompts
To test code completion prompts, choose one of the following methods.
REST
Vertex AI SDK for Python
Node.js
Java
Console
To test a code completion prompt with the Vertex AI API, send a POST request to the publisher model endpoint.
Before using any of the request data, make the following replacements:
PROJECT_ID: Your project ID.
PREFIX: For code models, prefix represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated. The model attempts to fill in the code in between the prefix and suffix.
SUFFIX: For code completion, suffix represents the end of a piece of meaningful programming code. The model attempts to fill in the code in between the prefix and suffix.
TEMPERATURE: The temperature is used for sampling during response generation. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.
MAX_OUTPUT_TOKENS: Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
Specify a lower value for shorter responses and a higher value for longer responses.
CANDIDATE_COUNT: The number of response variations to return. The range of valid values is an int between 1 and 4.
HTTP method and URL:
POST https://us-central1-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/us-central1/publishers/google/models/code-gecko:predict
Request JSON body:
{
  "instances": [
    { "prefix": "
PREFIX",
      "suffix": "
SUFFIX"}
  ],
  "parameters": {
    "temperature": 
TEMPERATURE,
    "maxOutputTokens": 
MAX_OUTPUT_TOKENS,
    "candidateCount": 
CANDIDATE_COUNT
  }
}
To send your request, choose one of these options:
curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login, or by using Cloud Shell, which automatically logs you into the gcloud CLI. You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:
curl -X POST \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    -H "Content-Type: application/json; charset=utf-8" \
    -d @request.json \
    "https://us-central1-aiplatform.googleapis.com/v1/projects/
PROJECT_ID/locations/us-central1/publishers/google/models/code-gecko:predict"








You should receive a JSON response similar to the following.
Response



























































































































































Example curl command
MODEL_ID="code-gecko"
PROJECT_ID=
PROJECT_ID

curl \
-X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:predict -d \
$"{
  'instances': [
      { 'prefix': 'def reverse_string(s):',
        'suffix': ''
    }
  ],
  'parameters': {
    'temperature': 0.2,
    'maxOutputTokens': 64,
    'candidateCount': 1
  }
}"
To learn more about prompt design for code completion, see Create prompts for code completion.
Stream response from code model
To view sample code requests and responses using the REST API, see Examples using the streaming REST API.
To view sample code requests and responses using the Vertex AI SDK for Python, see Examples using Vertex AI SDK for Python for streaming.
What's next
Learn how to create code completion prompts.
Learn how to create code generation prompts.
Learn about responsible AI best practices and Vertex AI's safety filters.
Was this helpful?
Send feedback